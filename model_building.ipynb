{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will build, test, and fine-tune all the models in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To use this notebook, create a virtual env and run pip install -r 'requirements.txt' before running the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### 1. [Datascrapping](#example)\n",
    "### 2. [VectorStore](#VectorStore)\n",
    "### 3. [Using the Model](#Model)\n",
    "### 4. [Fine-tuning](#Fine-tuning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything\n",
    "import langchain\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import faiss\n",
    "import openai\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import json\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to ur OPENAI key here\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a vectorstore from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your data first\n",
    "loader = CSVLoader(file_path=\"FinalOutput.csv\") #here's an example of a csv file\n",
    "data=loader.load()\n",
    "\n",
    "#chunking the data \n",
    "text_splitter = CharacterTextSplitter(separator='\\n',\n",
    "                                      chunk_size=1000,\n",
    "                                      chunk_overlap=200)\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "#initialize the openai embeddings system\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#Creating a new vector store from the documents \n",
    "VectorStore_openAI = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "#Saving the new vectorstore to be called 'faiss_store_demo'\n",
    "VectorStore_openAI.save_local(\"faiss_store_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = FAISS.load_local(\"faiss_store_demo\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new data to an existing VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = faiss.read_index(\"./faiss_store/index.faiss\")\n",
    "index2 = faiss.read_index(\"./faiss_store2/index.faiss\")\n",
    "print(index1.d == index2.d)\n",
    "new_index = faiss.IndexFlatL2(index1.d)  # Example for a flat L2 index\n",
    "\n",
    "# Add vectors from both indexes to the new index\n",
    "# This step varies based on index type and might require extracting vectors differently\n",
    "# Here, it's assumed you can directly add vectors\n",
    "new_index.add(index1.reconstruct_n(0, index1.ntotal))\n",
    "new_index.add(index2.reconstruct_n(0, index2.ntotal))\n",
    "faiss.write_index(new_index,'./faiss_store_merged/index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "#Create a training file first from the input\n",
    "client.files.create(\n",
    "  file=open(\"transformed_interactions.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "'''\n",
    "output: a training_file id from OpenAI, e.g.: 'file-ZsAaCWc0D5ceTsqEgr3ZdZbv'\n",
    "'''\n",
    "\n",
    "#Use the given training file id to create the finetuning job, and select the base model(could be a finetuned model)\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file='file-ZsAaCWc0D5ceTsqEgr3ZdZbv', \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "'''\n",
    "output: a job id form OpenAI, e.g.:'ftjob-BIOaKlGG3oALplrJCFarIu5N'\n",
    "'''\n",
    "\n",
    "#retrieve the job status\n",
    "client.fine_tuning.jobs.retrieve('ftjob-BIOaKlGG3oALplrJCFarIu5N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import chat_models\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the vector store to retrieve info from\n",
    "VectorStore = FAISS.load_local(\"VectorStores/faiss_store_merged2\", OpenAIEmbeddings(),allow_dangerous_deserialization=True)\n",
    "retriever = VectorStore.as_retriever()\n",
    "\n",
    "#Specifying the model\n",
    "llm  = chat_models.ChatOpenAI(model='gpt-4o')\n",
    "#Enabling chat history memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True, output_key='answer')\n",
    "#model\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory,get_chat_history=lambda h : h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'In the context of the course information provided, \"LCD\" stands for \"Liberal & Civic Discourse.\"'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"question\":\"What is LCD?\"},return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
